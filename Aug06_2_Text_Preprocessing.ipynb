{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKSoaMs81IKfWmkSoTPLWR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ean0418/ean0418/blob/main/Aug06_2_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing - 텍스트 전처리\n",
        "\n",
        "내가 해결하고자 하는 문제의 용도에 맞게 텍스트를 사전에 처리해버리는 작업\n",
        "\n"
      ],
      "metadata": {
        "id": "wNkSblWEE5ix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkp2YtZVEtM9",
        "outputId": "bb7a8a93-181f-443d-b59d-186091764ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk # 자연어 처리를 위한 패키지\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "nltk.download('punkt') # 문장 구조를 학습한 일종의 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화(Tokenization)\n",
        "\n",
        "어떤 문장을 단어로 잘라내서 정제하고, 정규화를 시키는 과정\n",
        "\n",
        "- 구두점 (Functiuation)\n",
        "  * 마침표, 쉼표, 물음표, 느낌표, 세미콜론, ...\n",
        "-"
      ],
      "metadata": {
        "id": "dm96PWk2F3Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"Slow and steady win the game. A friend in need is a friend indeed. The first property is the health.\""
      ],
      "metadata": {
        "id": "Bs1FH0AtFX7r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))\n",
        "# word_tokenize : Don't => Do와 n't / you're => you와 're로 구분\n",
        "print()\n",
        "print(WordPunctTokenizer().tokenize(text))\n",
        "# 구두점을 별도로 표시\n",
        "print()\n",
        "print(text_to_word_sequence(text))\n",
        "# keras의 text_to_word_sequence : 모든 알파벳을 소문자로 바꿔줌\n",
        "#                                 구두점 제거\n",
        "#                                 you're, don't,ain't 같은 경우는 보존함\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqrXmca6G2wP",
        "outputId": "ece06358-2dff-4e03-f382-15e78779c59b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Slow', 'and', 'steady', 'win', 'the', 'game', '.', 'A', 'friend', 'in', 'need', 'is', 'a', 'friend', 'indeed', '.', 'The', 'first', 'property', 'is', 'the', 'health', '.']\n",
            "\n",
            "['Slow', 'and', 'steady', 'win', 'the', 'game', '.', 'A', 'friend', 'in', 'need', 'is', 'a', 'friend', 'indeed', '.', 'The', 'first', 'property', 'is', 'the', 'health', '.']\n",
            "\n",
            "['slow', 'and', 'steady', 'win', 'the', 'game', 'a', 'friend', 'in', 'need', 'is', 'a', 'friend', 'indeed', 'the', 'first', 'property', 'is', 'the', 'health']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문장 토큰화(Sentence Tokenization)"
      ],
      "metadata": {
        "id": "HWPU7j8kNHxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence =\"\"\"ROMEO.\n",
        "He jests at scars that never felt a wound.\n",
        "\n",
        "Juliet appears above at a window.\n",
        "\n",
        "But soft, what light through yonder window breaks?\n",
        "It is the east, and Juliet is the sun!\n",
        "Arise fair sun and kill the envious moon,\n",
        "Who is already sick and pale with grief,\n",
        "That thou her maid art far more fair than she.\n",
        "Be not her maid since she is envious;\n",
        "Her vestal livery is but sick and green,\n",
        "And none but fools do wear it; cast it off.\n",
        "It is my lady, O it is my love!\n",
        "O, that she knew she were!\n",
        "She speaks, yet she says nothing. What of that?\n",
        "Her eye discourses, I will answer it.\n",
        "I am too bold, ’tis not to me she speaks.\n",
        "Two of the fairest stars in all the heaven,\n",
        "Having some business, do entreat her eyes\n",
        "To twinkle in their spheres till they return.\n",
        "What if her eyes were there, they in her head?\n",
        "The brightness of her cheek would shame those stars,\n",
        "As daylight doth a lamp; her eyes in heaven\n",
        "Would through the airy region stream so bright\n",
        "That birds would sing and think it were not night.\n",
        "See how she leans her cheek upon her hand.\n",
        "O that I were a glove upon that hand,\n",
        "That I might touch that cheek.\"\"\"\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "iEVfHM3VHJaL",
        "outputId": "2e7d155b-63b8-4568-cb3d-127dd0cb0fc9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ROMEO.\\nHe jests at scars that never felt a wound.\\n\\nJuliet appears above at a window.\\n\\nBut soft, what light through yonder window breaks?\\nIt is the east, and Juliet is the sun!\\nArise fair sun and kill the envious moon,\\nWho is already sick and pale with grief,\\nThat thou her maid art far more fair than she.\\nBe not her maid since she is envious;\\nHer vestal livery is but sick and green,\\nAnd none but fools do wear it; cast it off.\\nIt is my lady, O it is my love!\\nO, that she knew she were!\\nShe speaks, yet she says nothing. What of that?\\nHer eye discourses, I will answer it.\\nI am too bold, ’tis not to me she speaks.\\nTwo of the fairest stars in all the heaven,\\nHaving some business, do entreat her eyes\\nTo twinkle in their spheres till they return.\\nWhat if her eyes were there, they in her head?\\nThe brightness of her cheek would shame those stars,\\nAs daylight doth a lamp; her eyes in heaven\\nWould through the airy region stream so bright\\nThat birds would sing and think it were not night.\\nSee how she leans her cheek upon her hand.\\nO that I were a glove upon that hand,\\nThat I might touch that cheek.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(sentence)\n",
        "# NLTK는 단순하게 마침표로 문장을 구분하지 않음\n",
        "# Dr. , Mrs. Mr. 등 단어들은 마침표를 기분으로 해서 나뉘어지지 않음 => 성공적!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCzJf46OFAh",
        "outputId": "d7cf9301-7622-4652-9a42-5ae9f599dad6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ROMEO.',\n",
              " 'He jests at scars that never felt a wound.',\n",
              " 'Juliet appears above at a window.',\n",
              " 'But soft, what light through yonder window breaks?',\n",
              " 'It is the east, and Juliet is the sun!',\n",
              " 'Arise fair sun and kill the envious moon,\\nWho is already sick and pale with grief,\\nThat thou her maid art far more fair than she.',\n",
              " 'Be not her maid since she is envious;\\nHer vestal livery is but sick and green,\\nAnd none but fools do wear it; cast it off.',\n",
              " 'It is my lady, O it is my love!',\n",
              " 'O, that she knew she were!',\n",
              " 'She speaks, yet she says nothing.',\n",
              " 'What of that?',\n",
              " 'Her eye discourses, I will answer it.',\n",
              " 'I am too bold, ’tis not to me she speaks.',\n",
              " 'Two of the fairest stars in all the heaven,\\nHaving some business, do entreat her eyes\\nTo twinkle in their spheres till they return.',\n",
              " 'What if her eyes were there, they in her head?',\n",
              " 'The brightness of her cheek would shame those stars,\\nAs daylight doth a lamp; her eyes in heaven\\nWould through the airy region stream so bright\\nThat birds would sing and think it were not night.',\n",
              " 'See how she leans her cheek upon her hand.',\n",
              " 'O that I were a glove upon that hand,\\nThat I might touch that cheek.',\n",
              " 'JULIET.',\n",
              " 'Ay me.',\n",
              " 'ROMEO.',\n",
              " 'She speaks.',\n",
              " 'O speak again bright angel, for thou art\\nAs glorious to this night, being o’er my head,\\nAs is a winged messenger of heaven\\nUnto the white-upturned wondering eyes\\nOf mortals that fall back to gaze on him\\nWhen he bestrides the lazy-puffing clouds\\nAnd sails upon the bosom of the air.',\n",
              " 'JULIET.',\n",
              " 'O Romeo, Romeo, wherefore art thou Romeo?',\n",
              " 'Deny thy father and refuse thy name.',\n",
              " 'Or if thou wilt not, be but sworn my love,\\nAnd I’ll no longer be a Capulet.',\n",
              " 'ROMEO.',\n",
              " '[Aside.]',\n",
              " 'Shall I hear more, or shall I speak at this?',\n",
              " 'JULIET.',\n",
              " '’Tis but thy name that is my enemy;\\nThou art thyself, though not a Montague.',\n",
              " 'What’s Montague?',\n",
              " 'It is nor hand nor foot,\\nNor arm, nor face, nor any other part\\nBelonging to a man.',\n",
              " 'O be some other name.',\n",
              " 'What’s in a name?',\n",
              " 'That which we call a rose\\nBy any other name would smell as sweet;\\nSo Romeo would, were he not Romeo call’d,\\nRetain that dear perfection which he owes\\nWithout that title.',\n",
              " 'Romeo, doff thy name,\\nAnd for thy name, which is no part of thee,\\nTake all myself.',\n",
              " 'ROMEO.',\n",
              " 'I take thee at thy word.',\n",
              " 'Call me but love, and I’ll be new baptis’d;\\nHenceforth I never will be Romeo.',\n",
              " 'JULIET.',\n",
              " 'What man art thou that, thus bescreen’d in night\\nSo stumblest on my counsel?',\n",
              " 'ROMEO.',\n",
              " 'By a name\\nI know not how to tell thee who I am:\\nMy name, dear saint, is hateful to myself,\\nBecause it is an enemy to thee.',\n",
              " 'Had I it written, I would tear the word.',\n",
              " 'JULIET.',\n",
              " 'My ears have yet not drunk a hundred words\\nOf thy tongue’s utterance, yet I know the sound.',\n",
              " 'Art thou not Romeo, and a Montague?',\n",
              " 'ROMEO.',\n",
              " 'Neither, fair maid, if either thee dislike.',\n",
              " 'JULIET.',\n",
              " 'How cam’st thou hither, tell me, and wherefore?',\n",
              " 'The orchard walls are high and hard to climb,\\nAnd the place death, considering who thou art,\\nIf any of my kinsmen find thee here.',\n",
              " 'ROMEO.',\n",
              " 'With love’s light wings did I o’erperch these walls,\\nFor stony limits cannot hold love out,\\nAnd what love can do, that dares love attempt:\\nTherefore thy kinsmen are no stop to me.',\n",
              " 'JULIET.',\n",
              " 'If they do see thee, they will murder thee.',\n",
              " 'ROMEO.',\n",
              " 'Alack, there lies more peril in thine eye\\nThan twenty of their swords.',\n",
              " 'Look thou but sweet,\\nAnd I am proof against their enmity.',\n",
              " 'JULIET.',\n",
              " 'I would not for the world they saw thee here.',\n",
              " 'ROMEO.',\n",
              " 'I have night’s cloak to hide me from their eyes,\\nAnd but thou love me, let them find me here.',\n",
              " 'My life were better ended by their hate\\nThan death prorogued, wanting of thy love.',\n",
              " 'JULIET.',\n",
              " 'By whose direction found’st thou out this place?',\n",
              " 'ROMEO.',\n",
              " 'By love, that first did prompt me to enquire;\\nHe lent me counsel, and I lent him eyes.',\n",
              " 'I am no pilot; yet wert thou as far\\nAs that vast shore wash’d with the farthest sea,\\nI should adventure for such merchandise.',\n",
              " 'JULIET.',\n",
              " 'Thou knowest the mask of night is on my face,\\nElse would a maiden blush bepaint my cheek\\nFor that which thou hast heard me speak tonight.',\n",
              " 'Fain would I dwell on form, fain, fain deny\\nWhat I have spoke; but farewell compliment.',\n",
              " 'Dost thou love me?',\n",
              " 'I know thou wilt say Ay,\\nAnd I will take thy word.',\n",
              " 'Yet, if thou swear’st,\\nThou mayst prove false.',\n",
              " 'At lovers’ perjuries,\\nThey say Jove laughs.',\n",
              " 'O gentle Romeo,\\nIf thou dost love, pronounce it faithfully.',\n",
              " 'Or if thou thinkest I am too quickly won,\\nI’ll frown and be perverse, and say thee nay,\\nSo thou wilt woo.',\n",
              " 'But else, not for the world.',\n",
              " 'In truth, fair Montague, I am too fond;\\nAnd therefore thou mayst think my ’haviour light:\\nBut trust me, gentleman, I’ll prove more true\\nThan those that have more cunning to be strange.',\n",
              " 'I should have been more strange, I must confess,\\nBut that thou overheard’st, ere I was ’ware,\\nMy true-love passion; therefore pardon me,\\nAnd not impute this yielding to light love,\\nWhich the dark night hath so discovered.',\n",
              " 'ROMEO.',\n",
              " 'Lady, by yonder blessed moon I vow,\\nThat tips with silver all these fruit-tree tops,—\\n\\nJULIET.',\n",
              " 'O swear not by the moon, th’inconstant moon,\\nThat monthly changes in her circled orb,\\nLest that thy love prove likewise variable.',\n",
              " 'ROMEO.',\n",
              " 'What shall I swear by?',\n",
              " 'JULIET.',\n",
              " 'Do not swear at all.',\n",
              " 'Or if thou wilt, swear by thy gracious self,\\nWhich is the god of my idolatry,\\nAnd I’ll believe thee.',\n",
              " 'ROMEO.',\n",
              " 'If my heart’s dear love,—']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS(Korean Sentence Splitter)\n",
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4r-aZEnOLMe",
        "outputId": "c4eada3e-1008-4a7c-8868-869cee5f72e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-6.0.4.tar.gz (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.3)\n",
            "Collecting jamo (from kss)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting hangul-jamo (from kss)\n",
            "  Downloading hangul_jamo-1.0.1-py3-none-any.whl.metadata (899 bytes)\n",
            "Collecting tossi (from kss)\n",
            "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting distance (from kss)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml==6.0 (from kss)\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting unidecode (from kss)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting cmudict (from kss)\n",
            "  Downloading cmudict-1.0.27-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting koparadigm (from kss)\n",
            "  Downloading koparadigm-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting kollocate (from kss)\n",
            "  Downloading kollocate-0.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting bs4 (from kss)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kss) (1.26.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from kss) (7.4.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kss) (1.13.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->kss) (4.12.3)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->kss) (8.2.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->kss) (6.4.0)\n",
            "Collecting whoosh (from kollocate->kss)\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting xlrd==1.2.0 (from koparadigm->kss)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (14.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (2024.5.15)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (2.0.1)\n",
            "Requirement already satisfied: bidict in /usr/local/lib/python3.10/dist-packages (from tossi->kss) (0.23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tossi->kss) (1.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5->cmudict->kss) (3.19.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->kss) (2.5)\n",
            "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading cmudict-1.0.27-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kss, distance, pecab, tossi\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-6.0.4-cp310-cp310-linux_x86_64.whl size=1446487 sha256=488b245bf66bcc60b15d337e5b1b6bfd5df17f7936a363ff2989d5ea90c39660\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/70/d5/c9308346829b1eb9e7267d74696919d2453aee6ce350f98b3b\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16257 sha256=b4aabd4da5154647e69a7c94b79a87748a40f1e4b98e0ceffa0a564c6cb06d8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646664 sha256=2744590b7d9744a290f0d1648fbf6678f31c94eef4eaef274590acc4883a4efb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
            "  Building wheel for tossi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12128 sha256=47a7a50071755a6073100a2e9588b6794a3a5bfa97a483189a23494738c4d506\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/18/60/1094a6fe93c8063efcd3e6700d09328216682e495a3c51af9f\n",
            "Successfully built kss distance pecab tossi\n",
            "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, tossi, pyyaml, kollocate, pecab, koparadigm, cmudict, bs4, kss\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "Successfully installed bs4-0.0.2 cmudict-1.0.27 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 jamo-0.4.1 kollocate-0.0.2 koparadigm-0.10.0 kss-6.0.4 pecab-1.0.8 pyyaml-6.0 tossi-0.3.1 unidecode-1.3.8 whoosh-2.7.4 xlrd-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss"
      ],
      "metadata": {
        "id": "yOs_xz5oPBDB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kor = \"오늘부터 AI 시작이에요. 텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높아요. 한번 경험해봅시다\"\n",
        "kor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_eVmrDLPQXWp",
        "outputId": "ed3ad06c-5936-4593-91f8-4eddfaa49935"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'오늘부터 AI 시작이에요. 텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높아요. 한번 경험해봅시다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(kss.split_sentences(kor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehGaSeJ5QeET",
        "outputId": "4ceb3dcf-ad98-4b99-c655-cb3162d986c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘부터 AI 시작이에요.', '텍스트 전처리는 한국어가 영어보다 훨씬 난이도가 높아요.', '한번 경험해봅시다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 = 교착어(어근 + 접사)\n",
        "\n",
        "한국어에는 [조사]가 존재\n",
        "\n",
        "- 글자 뒤에 띄어쓰기 없이 존재\n",
        "- 형태소 (morpheme)\n",
        "  -> 말의 가장 작은 단위\n",
        "    - 자립형태소 : 명사, 대명사, 수사, 관형사, 부사, ...\n",
        "    - 의존형태소 : 다른 형태소와 결합을 해야만하는... 어간, 어미, 접소, 조사, ..."
      ],
      "metadata": {
        "id": "8a0HYmOqQ43H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 품사 태킹(Part-of-speech tagging) : 단어 토큰화를 거친 토큰들(단어들)에게 품사를 붙여주는 작업\n",
        "\n",
        "동음이의어\n",
        "\n",
        "mean : 동사] 의미하다/ 명사] 평균 / 형용사] 비열한, 못된\n",
        "\n",
        "연패 : 연속해서 패하다 / 연속해서 이기다\n",
        "\n"
      ],
      "metadata": {
        "id": "En4De7riRvBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK / KoNLPy"
      ],
      "metadata": {
        "id": "DDWSlJCWSaAI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger') # 품사태깅을 위한 라이브러리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZmkUy2ISeGL",
        "outputId": "14f1e506-10ce-417e-9335-1a46947730ae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag"
      ],
      "metadata": {
        "id": "uDKnFy6ISmXY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"Slow and steady win the game. A friend in need is a friend indeed. The first property is the health.\"\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zUTdPX7gSqh1",
        "outputId": "ccf47ea4-50a9-4fcf-9fe2-29ed92e25a0c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Slow and steady win the game. A friend in need is a friend indeed. The first property is the health.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = word_tokenize(text)\n",
        "print(tokenized_sentence)\n",
        "print(pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tog4eQxkS3o9",
        "outputId": "e3846d51-cbc7-4521-db7c-6c5572ec88a9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Slow', 'and', 'steady', 'win', 'the', 'game', '.', 'A', 'friend', 'in', 'need', 'is', 'a', 'friend', 'indeed', '.', 'The', 'first', 'property', 'is', 'the', 'health', '.']\n",
            "[('Slow', 'NNP'), ('and', 'CC'), ('steady', 'JJ'), ('win', 'VB'), ('the', 'DT'), ('game', 'NN'), ('.', '.'), ('A', 'DT'), ('friend', 'NN'), ('in', 'IN'), ('need', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('friend', 'NN'), ('indeed', 'RB'), ('.', '.'), ('The', 'DT'), ('first', 'JJ'), ('property', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('health', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRP : 인칭대명사\n",
        "# RB : 부사\n",
        "# DT : 관사\n",
        "# VBP : 단수, 현재형, 3인칭 동사\n",
        "# W~ : wh~\n",
        "# JJ : 형용사\n",
        "# NN : 단수명사\n",
        "# NNS : 복수명사\n",
        "# MD : 조동사\n",
        "# VB : 동사 기본형\n",
        "# VBD : 동사 과거시제\n",
        "# VBG : 동명사\n"
      ],
      "metadata": {
        "id": "-nB_Wp7ETU28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 자연어처리 : KoNLPy라는 파이썬 패키지\n",
        "\n",
        "KoNLPy에서 사용할 수 있는 형태소 분석기\n",
        "- Okt (Open Korean Text)\n",
        "- Komoran\n",
        "- kkma\n",
        "- Mecab\n",
        "- Hannanum\n"
      ],
      "metadata": {
        "id": "1q2CseTQT8UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b5sXvrgTFGq",
        "outputId": "6edcdec2-9867-48b4-c9ff-6125bd326ff5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma"
      ],
      "metadata": {
        "id": "rne7pcL4UV_V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "print(okt.morphs(\"오늘은 화요일이고요. 내일은 없습니다!\"))\n",
        "# morphs : 형태소 분석 : 어떤 대상의 어절을 최소 의미단위인 형태소로 분석하는 것\n",
        "print(okt.pos(\"오늘은 화요일이고요. 내일은 없습니다!\"))\n",
        "# pos : 품사 태깅(Part-of-Speech tagging)\n",
        "print(okt.nouns(\"오늘은 화요일이고요. 내일은 없습니다!\"))\n",
        "# nouns : 명사 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usgbHBDCUiWj",
        "outputId": "206c212b-db23-4351-f297-56f1947da74f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘', '은', '화요일', '이', '고요', '.', '내일', '은', '없습니다', '!']\n",
            "[('오늘', 'Noun'), ('은', 'Josa'), ('화요일', 'Noun'), ('이', 'Josa'), ('고요', 'Noun'), ('.', 'Punctuation'), ('내일', 'Noun'), ('은', 'Josa'), ('없습니다', 'Adjective'), ('!', 'Punctuation')]\n",
            "['오늘', '화요일', '고요', '내일']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma = Kkma()\n",
        "print(kkma.morphs(\"오늘은 화요일이고요. 내일은 없습니다!\"))\n",
        "# morphs : 형태소 분석 : 어떤 대상의 어절을 최소 의미단위인 형태소로 분석하는 것\n",
        "print(kkma.pos(\"오늘은 화요일이고요. 내일은 없습니다!\"))\n",
        "# pos : 품사 태깅(Part-of-Speech tagging)\n",
        "print(kkma.nouns(\"오늘은 화요일이고요. 내일은 없습니다!\"))\n",
        "# nouns : 명사 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsz9kEVOU2kn",
        "outputId": "490932e8-1006-4aa3-d2be-a6079d9fdff8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘', '은', '화요일', '이', '고요', '.', '내일', '은', '없', '습니다', '!']\n",
            "[('오늘', 'NNG'), ('은', 'JX'), ('화요일', 'NNG'), ('이', 'VCP'), ('고요', 'EFN'), ('.', 'SF'), ('내일', 'NNG'), ('은', 'JX'), ('없', 'VA'), ('습니다', 'EFN'), ('!', 'SF')]\n",
            "['오늘', '화요일', '내일']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 코퍼스(Coupus) : 말뭉치\n",
        "\n",
        "보통 여러 단어들로 이루어진 문장, 분석하려는 대상, 문서, 데이터셋\n",
        "\n",
        "코퍼스에서 용도에 맞게 토큰을 나누는 것을 토큰화(Tokenization), 정규화(Normalization)를 하는 것이 필요 !\n",
        "\n",
        "- 정제(Cleaning) : 가지고 있는 말뭉치에서 노이즈 데이터를 제거\n",
        "- 정규화(Normalization) : 표현 방법이 서로 다른 단어들을 통일시켜서 같은 단어로 재가공\n",
        "  1. 규칙에 따라서 표기가 다른 언어를 통합시키기\n",
        "  ex) US USA us U.S.A\n",
        "  2. 대소문자를 통합"
      ],
      "metadata": {
        "id": "ybv0QPrdWnip"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-LozU04Vhj8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}