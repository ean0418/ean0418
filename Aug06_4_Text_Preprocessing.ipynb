{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS9Ovbq6L+s77+mbFiC92T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ean0418/ean0418/blob/main/Aug06_4_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 표제어 추출 (Lemmatization)\n",
        "\n",
        "말뭉치(코퍼스)의 단어의 갯수를 줄일 수 있는 기법\n",
        "\n",
        "be동사 : be, am, are, is\n",
        "\n",
        "공부하다 : 공부하고, 공부때문에, 공부여서, 공부덕분에, ...\n",
        "\n",
        "- 분석시에 단어 빈도수 기반으로 진행 => 자연어 처리 단계에서 상당히 자주 사용\n",
        "\n",
        "- 형태소로부터 단어를 만들어가는...\n",
        "  - 어간(stem) : 의미가 있는 단어의 핵심부분\n",
        "  - 접사(affix) : 단어에 추가적인 의미를 부여하는 부분\n",
        "\n",
        "  형태학적 파싱 : 코퍼스에서 어간과 접사를 분리하는 것  \n"
      ],
      "metadata": {
        "id": "gS12arUWGkkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj_KuS5TGZ-j",
        "outputId": "a1b62317-1c9d-4a5e-a567-cf2b969fb331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNetLemmatizer : NLTK에서 지원하는 표제어 추출 도구\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DDww97xH7As",
        "outputId": "09d3b673-0461-437d-e696-ab90fbf9b21e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words =['sky','computer','having','lives','love','mouse','dies','listened','ate','has']\n",
        "print('추출 전:', words)\n",
        "print('추출 후:', [lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA8twFHUIayb",
        "outputId": "f12dbe3f-3659-44dd-e5d6-c062418ba574"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "추출 전: ['sky', 'computer', 'having', 'lives', 'love', 'mouse', 'dies', 'listened', 'ate', 'has']\n",
            "추출 후: ['sky', 'computer', 'having', 'life', 'love', 'mouse', 'dy', 'listened', 'ate', 'ha']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('dies','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tdBMUoOhIri5",
        "outputId": "56356c64-efdb-432d-ef83-d3fc4e55c74f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'die'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('better', 'a')\n",
        "\n",
        "# v : 동사, / a : 형용사 / n : 명사 / r : 부사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yIGDOgogJPrq",
        "outputId": "c056742e-1c1a-42fd-807f-e7c3602fcefe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR-YOBtzJpt0",
        "outputId": "320822a8-2cbf-4eec-fa4f-09e1857c4478"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "dBjm8X2GJ7qg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"\"\"Thou knowest the mask of night is on my face,\n",
        "Else would a maiden blush bepaint my cheek\n",
        "For that which thou hast heard me speak tonight.\n",
        "Fain would I dwell on form, fain, fain deny\n",
        "What I have spoke; but farewell compliment.\n",
        "Dost thou love me? I know thou wilt say Ay,\n",
        "And I will take thy word. Yet, if thou swear’st,\n",
        "Thou mayst prove false. At lovers’ perjuries,\n",
        "They say Jove laughs. O gentle Romeo,\n",
        "If thou dost love, pronounce it faithfully.\n",
        "Or if thou thinkest I am too quickly won,\n",
        "I’ll frown and be perverse, and say thee nay,\n",
        "So thou wilt woo. But else, not for the world.\n",
        "In truth, fair Montague, I am too fond;\n",
        "And therefore thou mayst think my ’haviour light:\n",
        "But trust me, gentleman, I’ll prove more true\n",
        "Than those that have more cunning to be strange.\n",
        "I should have been more strange, I must confess,\n",
        "But that thou overheard’st, ere I was ’ware,\n",
        "My true-love passion; therefore pardon me,\n",
        "And not impute this yielding to light love,\n",
        "Which the dark night hath so discovered.\"\"\""
      ],
      "metadata": {
        "id": "19USyPcSKAkB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "words2 = word_tokenize(sentence)\n",
        "print(words2)\n",
        "print([stemmer.stem(w) for w in words2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuUzVOucKVSp",
        "outputId": "968969a7-65fc-45f7-c65a-b35cae7d6309"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Thou', 'knowest', 'the', 'mask', 'of', 'night', 'is', 'on', 'my', 'face', ',', 'Else', 'would', 'a', 'maiden', 'blush', 'bepaint', 'my', 'cheek', 'For', 'that', 'which', 'thou', 'hast', 'heard', 'me', 'speak', 'tonight', '.', 'Fain', 'would', 'I', 'dwell', 'on', 'form', ',', 'fain', ',', 'fain', 'deny', 'What', 'I', 'have', 'spoke', ';', 'but', 'farewell', 'compliment', '.', 'Dost', 'thou', 'love', 'me', '?', 'I', 'know', 'thou', 'wilt', 'say', 'Ay', ',', 'And', 'I', 'will', 'take', 'thy', 'word', '.', 'Yet', ',', 'if', 'thou', 'swear', '’', 'st', ',', 'Thou', 'mayst', 'prove', 'false', '.', 'At', 'lovers', '’', 'perjuries', ',', 'They', 'say', 'Jove', 'laughs', '.', 'O', 'gentle', 'Romeo', ',', 'If', 'thou', 'dost', 'love', ',', 'pronounce', 'it', 'faithfully', '.', 'Or', 'if', 'thou', 'thinkest', 'I', 'am', 'too', 'quickly', 'won', ',', 'I', '’', 'll', 'frown', 'and', 'be', 'perverse', ',', 'and', 'say', 'thee', 'nay', ',', 'So', 'thou', 'wilt', 'woo', '.', 'But', 'else', ',', 'not', 'for', 'the', 'world', '.', 'In', 'truth', ',', 'fair', 'Montague', ',', 'I', 'am', 'too', 'fond', ';', 'And', 'therefore', 'thou', 'mayst', 'think', 'my', '’', 'haviour', 'light', ':', 'But', 'trust', 'me', ',', 'gentleman', ',', 'I', '’', 'll', 'prove', 'more', 'true', 'Than', 'those', 'that', 'have', 'more', 'cunning', 'to', 'be', 'strange', '.', 'I', 'should', 'have', 'been', 'more', 'strange', ',', 'I', 'must', 'confess', ',', 'But', 'that', 'thou', 'overheard', '’', 'st', ',', 'ere', 'I', 'was', '’', 'ware', ',', 'My', 'true-love', 'passion', ';', 'therefore', 'pardon', 'me', ',', 'And', 'not', 'impute', 'this', 'yielding', 'to', 'light', 'love', ',', 'Which', 'the', 'dark', 'night', 'hath', 'so', 'discovered', '.']\n",
            "['thou', 'knowest', 'the', 'mask', 'of', 'night', 'is', 'on', 'my', 'face', ',', 'els', 'would', 'a', 'maiden', 'blush', 'bepaint', 'my', 'cheek', 'for', 'that', 'which', 'thou', 'hast', 'heard', 'me', 'speak', 'tonight', '.', 'fain', 'would', 'i', 'dwell', 'on', 'form', ',', 'fain', ',', 'fain', 'deni', 'what', 'i', 'have', 'spoke', ';', 'but', 'farewel', 'compliment', '.', 'dost', 'thou', 'love', 'me', '?', 'i', 'know', 'thou', 'wilt', 'say', 'ay', ',', 'and', 'i', 'will', 'take', 'thi', 'word', '.', 'yet', ',', 'if', 'thou', 'swear', '’', 'st', ',', 'thou', 'mayst', 'prove', 'fals', '.', 'at', 'lover', '’', 'perjuri', ',', 'they', 'say', 'jove', 'laugh', '.', 'o', 'gentl', 'romeo', ',', 'if', 'thou', 'dost', 'love', ',', 'pronounc', 'it', 'faith', '.', 'or', 'if', 'thou', 'thinkest', 'i', 'am', 'too', 'quickli', 'won', ',', 'i', '’', 'll', 'frown', 'and', 'be', 'pervers', ',', 'and', 'say', 'thee', 'nay', ',', 'so', 'thou', 'wilt', 'woo', '.', 'but', 'els', ',', 'not', 'for', 'the', 'world', '.', 'in', 'truth', ',', 'fair', 'montagu', ',', 'i', 'am', 'too', 'fond', ';', 'and', 'therefor', 'thou', 'mayst', 'think', 'my', '’', 'haviour', 'light', ':', 'but', 'trust', 'me', ',', 'gentleman', ',', 'i', '’', 'll', 'prove', 'more', 'true', 'than', 'those', 'that', 'have', 'more', 'cun', 'to', 'be', 'strang', '.', 'i', 'should', 'have', 'been', 'more', 'strang', ',', 'i', 'must', 'confess', ',', 'but', 'that', 'thou', 'overheard', '’', 'st', ',', 'ere', 'i', 'wa', '’', 'ware', ',', 'my', 'true-lov', 'passion', ';', 'therefor', 'pardon', 'me', ',', 'and', 'not', 'imput', 'thi', 'yield', 'to', 'light', 'love', ',', 'which', 'the', 'dark', 'night', 'hath', 'so', 'discov', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PorterStemmer : 알고리즘\n",
        "\n",
        "규칙 기반의 접근 => 어림짐작하는 작업 => 섬세한 작업 X => 사전에 없는 단어가 도출될수도...!\n",
        "\n",
        "- 마틴포터 홈페이지에서 다양한 것들을 살펴볼 수 있음\n",
        "- 규칙 기반의 접근\n",
        "  - -ALIZE = -AL\n",
        "  - -ANCE => 삭제\n",
        "  - -ICAL => -IC"
      ],
      "metadata": {
        "id": "17HZ96HnKwgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = ['channelize', 'allowance', 'typical']\n",
        "\n",
        "print('추출 전 ', word)\n",
        "print('추출 후 ', [stemmer.stem(w) for w in word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qwz9QyvKifx",
        "outputId": "9281a88d-3867-4fd1-e16d-9cb3df09063f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "추출 전  ['channelize', 'allowance', 'typical']\n",
            "추출 후  ['channel', 'allow', 'typic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원\n",
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "print([stemmer.stem(w) for w in words])\n",
        "print([lancaster.stem(w) for w in words])\n",
        "\n",
        "# 두 스태머가 다른 결과를 보여줌\n",
        "# 두 스태머는 서로 다른 알고리즘을 사용하기 때문!\n",
        "# 제대로 된 표제어를 뽑아오지는 못하고 있음\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tOPj9VzLoVH",
        "outputId": "8a9f3300-3ebd-4e9a-b7c2-c81979374a85"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sky', 'comput', 'have', 'live', 'love', 'mous', 'die', 'listen', 'ate', 'ha']\n",
            "['sky', 'comput', 'hav', 'liv', 'lov', 'mous', 'die', 'list', 'at', 'has']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 불용어 (Stopword)\n",
        "\n",
        "단어들 중에서 의미가 없는 단어\n",
        "\n",
        "데이터 중에서 의미가 있는 단어 토큰만 취급하기위해서 의미를 가지지 않은 단어들을 제거하는 작업"
      ],
      "metadata": {
        "id": "Iw94BqOmMyFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag0wAzjzL8vE",
        "outputId": "eed53d95-1746-4597-a3e9-3e4508a9ffa2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "b8nLHUTQNGep"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에 있는 불용어\n",
        "s = stopwords.words('english')\n",
        "print(len(s))\n",
        "print(s[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwNAsJ7GNJ2V",
        "outputId": "3a85115c-53bf-45d6-875d-4397b90a4ae2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"\"\"Thou knowest the mask of night is on my face,\n",
        "Else would a maiden blush bepaint my cheek\n",
        "For that which thou hast heard me speak tonight.\n",
        "Fain would I dwell on form, fain, fain deny\n",
        "What I have spoke; but farewell compliment.\n",
        "Dost thou love me? I know thou wilt say Ay,\n",
        "And I will take thy word. Yet, if thou swear’st,\n",
        "Thou mayst prove false. At lovers’ perjuries,\n",
        "They say Jove laughs. O gentle Romeo,\n",
        "If thou dost love, pronounce it faithfully.\n",
        "Or if thou thinkest I am too quickly won,\n",
        "I’ll frown and be perverse, and say thee nay,\n",
        "So thou wilt woo. But else, not for the world.\n",
        "In truth, fair Montague, I am too fond;\n",
        "And therefore thou mayst think my ’haviour light:\n",
        "But trust me, gentleman, I’ll prove more true\n",
        "Than those that have more cunning to be strange.\n",
        "I should have been more strange, I must confess,\n",
        "But that thou overheard’st, ere I was ’ware,\n",
        "My true-love passion; therefore pardon me,\n",
        "And not impute this yielding to light love,\n",
        "Which the dark night hath so discovered.\"\"\"\n",
        "\n",
        "# NLTK에서 지정한 불용어 가져오기\n",
        "sw = set(stopwords.words('english'))\n",
        "\n",
        "# 문장을 단어로 쪼개는 작업\n",
        "word = word_tokenize(sentence)\n",
        "\n",
        "# 불용어가 아닌 단어들만 list에 담아서 출력\n",
        "result=[]\n",
        "for w in word:\n",
        "  if w not in sw:\n",
        "    result.append(w)\n",
        "\n",
        "print('불용어 제거 전 :', word)\n",
        "print('불용어 제거 후 :', result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ailFA7W2NSB4",
        "outputId": "beb79d8c-6235-4b9b-b277-655a47c2a090"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Thou', 'knowest', 'the', 'mask', 'of', 'night', 'is', 'on', 'my', 'face', ',', 'Else', 'would', 'a', 'maiden', 'blush', 'bepaint', 'my', 'cheek', 'For', 'that', 'which', 'thou', 'hast', 'heard', 'me', 'speak', 'tonight', '.', 'Fain', 'would', 'I', 'dwell', 'on', 'form', ',', 'fain', ',', 'fain', 'deny', 'What', 'I', 'have', 'spoke', ';', 'but', 'farewell', 'compliment', '.', 'Dost', 'thou', 'love', 'me', '?', 'I', 'know', 'thou', 'wilt', 'say', 'Ay', ',', 'And', 'I', 'will', 'take', 'thy', 'word', '.', 'Yet', ',', 'if', 'thou', 'swear', '’', 'st', ',', 'Thou', 'mayst', 'prove', 'false', '.', 'At', 'lovers', '’', 'perjuries', ',', 'They', 'say', 'Jove', 'laughs', '.', 'O', 'gentle', 'Romeo', ',', 'If', 'thou', 'dost', 'love', ',', 'pronounce', 'it', 'faithfully', '.', 'Or', 'if', 'thou', 'thinkest', 'I', 'am', 'too', 'quickly', 'won', ',', 'I', '’', 'll', 'frown', 'and', 'be', 'perverse', ',', 'and', 'say', 'thee', 'nay', ',', 'So', 'thou', 'wilt', 'woo', '.', 'But', 'else', ',', 'not', 'for', 'the', 'world', '.', 'In', 'truth', ',', 'fair', 'Montague', ',', 'I', 'am', 'too', 'fond', ';', 'And', 'therefore', 'thou', 'mayst', 'think', 'my', '’', 'haviour', 'light', ':', 'But', 'trust', 'me', ',', 'gentleman', ',', 'I', '’', 'll', 'prove', 'more', 'true', 'Than', 'those', 'that', 'have', 'more', 'cunning', 'to', 'be', 'strange', '.', 'I', 'should', 'have', 'been', 'more', 'strange', ',', 'I', 'must', 'confess', ',', 'But', 'that', 'thou', 'overheard', '’', 'st', ',', 'ere', 'I', 'was', '’', 'ware', ',', 'My', 'true-love', 'passion', ';', 'therefore', 'pardon', 'me', ',', 'And', 'not', 'impute', 'this', 'yielding', 'to', 'light', 'love', ',', 'Which', 'the', 'dark', 'night', 'hath', 'so', 'discovered', '.']\n",
            "불용어 제거 후 : ['Thou', 'knowest', 'mask', 'night', 'face', ',', 'Else', 'would', 'maiden', 'blush', 'bepaint', 'cheek', 'For', 'thou', 'hast', 'heard', 'speak', 'tonight', '.', 'Fain', 'would', 'I', 'dwell', 'form', ',', 'fain', ',', 'fain', 'deny', 'What', 'I', 'spoke', ';', 'farewell', 'compliment', '.', 'Dost', 'thou', 'love', '?', 'I', 'know', 'thou', 'wilt', 'say', 'Ay', ',', 'And', 'I', 'take', 'thy', 'word', '.', 'Yet', ',', 'thou', 'swear', '’', 'st', ',', 'Thou', 'mayst', 'prove', 'false', '.', 'At', 'lovers', '’', 'perjuries', ',', 'They', 'say', 'Jove', 'laughs', '.', 'O', 'gentle', 'Romeo', ',', 'If', 'thou', 'dost', 'love', ',', 'pronounce', 'faithfully', '.', 'Or', 'thou', 'thinkest', 'I', 'quickly', ',', 'I', '’', 'frown', 'perverse', ',', 'say', 'thee', 'nay', ',', 'So', 'thou', 'wilt', 'woo', '.', 'But', 'else', ',', 'world', '.', 'In', 'truth', ',', 'fair', 'Montague', ',', 'I', 'fond', ';', 'And', 'therefore', 'thou', 'mayst', 'think', '’', 'haviour', 'light', ':', 'But', 'trust', ',', 'gentleman', ',', 'I', '’', 'prove', 'true', 'Than', 'cunning', 'strange', '.', 'I', 'strange', ',', 'I', 'must', 'confess', ',', 'But', 'thou', 'overheard', '’', 'st', ',', 'ere', 'I', '’', 'ware', ',', 'My', 'true-love', 'passion', ';', 'therefore', 'pardon', ',', 'And', 'impute', 'yielding', 'light', 'love', ',', 'Which', 'dark', 'night', 'hath', 'discovered', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 불용어 제거하기\n",
        "\n",
        "- 토큰화 => 조사 or 접속사 같이 명사 or\n",
        "형용사에서 필요없는 단어들을 제거\n",
        "\n",
        "- 한국어의 경우에는 사용자가 직접 불용어를 지정해서 사용하는 경우가 많음"
      ],
      "metadata": {
        "id": "I9KamLnTSCGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ptGWlGJN7ul",
        "outputId": "cf5ff1f0-688b-4b29-f0ad-a87f6bdb4b54"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from Konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from Konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from Konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->Konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, Konlpy\n",
            "Successfully installed JPype1-1.5.0 Konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "Kp0HDFVrSR-n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "ex = \"점심 먹고 나서 피곤하시죠? 여러분은 무슨 메뉴를 드셨나요? 저는 카이센동을 먹었습니다.\"\n",
        "sw = \"를 무슨 는 은 을\"\n",
        "\n",
        "sw = set(sw.split(' '))\n",
        "\n",
        "token = okt.morphs(ex)\n",
        "\n",
        "result = [w for w in token if not w in sw]\n",
        "\n",
        "print(token)\n",
        "\n",
        "print()\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj6nRc0NSocJ",
        "outputId": "f2885db4-4313-41ec-b22d-9e0843552ccb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['점심', '먹고', '나서', '피곤하시죠', '?', '여러분', '은', '무슨', '메뉴', '를', '드셨나요', '?', '저', '는', '카이센동', '을', '먹었습니다', '.']\n",
            "\n",
            "['점심', '먹고', '나서', '피곤하시죠', '?', '여러분', '메뉴', '드셨나요', '?', '저', '카이센동', '먹었습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정수 인코딩 (Integer Encoding)\n",
        "\n",
        "컴퓨터의 입장에서는 텍스트보다는 숫자를 더 쉽게 처리하는 경향이 있음\n",
        "\n",
        "텍스트에 정수를 부여하는 방법\n",
        "1. 단어를 빈도수 순으로 정렬\n",
        "2. 정렬된 집합 구성\n",
        "3. 빈도수가 높은 순서대로 낮은 숫자부터 정수를 부여"
      ],
      "metadata": {
        "id": "sEJrj9LETYCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어 동요\n",
        "text =\"\"\"Twinkle, twinkle little star.\n",
        "How I wonder what you are.\n",
        "Up above the world so high.\n",
        "Like a diamond in the sky.\n",
        "Twinkle, twinkle little star.\n",
        "How I wonder what you are.\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "WlAjQTVATmq7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize # 영어 문장 토큰화\n",
        "from nltk.tokenize import word_tokenize # 영어 단어 토큰화\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "J2T6KIPOUlJm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sentence = sent_tokenize(text)\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vWb3yNkUtTk",
        "outputId": "bf8e1907-9d8e-4172-c573-10f47d1aceb4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Twinkle, twinkle little star.',\n",
              " 'How I wonder what you are.',\n",
              " 'Up above the world so high.',\n",
              " 'Like a diamond in the sky.',\n",
              " 'Twinkle, twinkle little star.',\n",
              " 'How I wonder what you are.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화 => 불용어를 뺀 단어 토큰들을 list에 담기\n",
        "sw = set(stopwords.words('english'))\n",
        "final_sentence =[]\n",
        "aa = {}\n",
        "\n",
        "for s in sentence:\n",
        "  # 단어 토큰화\n",
        "  word = word_tokenize(s)\n",
        "  result = []\n",
        "  for w in word:\n",
        "    w = w.lower() # 모든 단어를 소문자화 => 단어 갯수를 줄이는데 도움O\n",
        "    if w not in sw:\n",
        "      if len(w) >2:\n",
        "        result.append(w)\n",
        "        if w not in aa:\n",
        "           aa[w] = 0\n",
        "        aa[w] +=1\n",
        "  final_sentence.append(result)\n",
        "print(final_sentence)\n",
        "print(aa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJHbZK5wU8VM",
        "outputId": "caa1dec0-16af-4634-b948-e9c397e3ed3d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['world', 'high'], ['like', 'diamond', 'sky'], ['twinkle', 'twinkle', 'little', 'star'], ['wonder']]\n",
            "{'twinkle': 4, 'little': 2, 'star': 2, 'wonder': 2, 'world': 1, 'high': 1, 'like': 1, 'diamond': 1, 'sky': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'little' 단어의 빈도수\n",
        "print(aa['little'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JzCy2RAVFYK",
        "outputId": "0608f371-174e-4be4-8733-5bcbbc0aa2bb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sorted() 함수 : 빈도수대로 정렬\n",
        "# sorted(정렬할 데이터, key옵션, reverse옵션)\n",
        "#     key 옵션 : 어떤 것을 기준으로 정렬할지 (key에 준 값으로 정렬)\n",
        "#     reverse옵션 : False(default) >> 오름차순\n",
        "\n",
        "# sort() vs sorted() :\n",
        "#   sort()는 리스트 자체를 정렬해서 바꾸는 형태\n",
        "#   sorted()는 원래 리스트는 그대로 두고, 정렬한 것을 새로운 리스트에 넣는 형태\n",
        "\n",
        "# key = lambda x : x[1] => x[1]의 값이 정렬의 기준 => 빈도수를 기준으로 정렬\n",
        "\n",
        "aaSort = sorted(aa.items(), key=lambda x:x[1], reverse=True)\n",
        "print(aaSort)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiiNtP7EWfot",
        "outputId": "f3438b2b-18aa-4653-8435-428dfc946656"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('twinkle', 4), ('little', 2), ('star', 2), ('wonder', 2), ('world', 1), ('high', 1), ('like', 1), ('diamond', 1), ('sky', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [높은 빈도수]를 가지고 있는 단어일수록 [낮은 정수값]을 부여 (정수는 1부터 부여)\n",
        "\n",
        "# 빈도수가 1이하인것들은 삭제 (결과에 안나오게)\n",
        "# {'twinkle' : 1, 'little' : 2, 'star' : 3, ...}\n",
        "\n",
        "aa_index = {}\n",
        "i = 0\n",
        "for (word, freq) in aaSort:\n",
        "  if freq > 1:\n",
        "    i += 1\n",
        "    aa_index[word] = i\n",
        "\n",
        "print(aa_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr8ve2OyXRzk",
        "outputId": "eebd60fc-4a28-447a-c880-97e8a1a59ee3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'twinkle': 1, 'little': 2, 'star': 3, 'wonder': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 빈도수가 가장 높은 상위 5개 출력\n",
        "freSize = 4\n",
        "\n",
        "# 인덱스가 4초과(5이상)인 단어들을 aa_final이라는 변수에 담기\n",
        "aa_final = [w for (w,index) in aa_index.items() if index >= freSize + 1]\n",
        "\n",
        "for w in aa_final:\n",
        "  del aa_index[w]\n",
        "print(aa_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_eiPM3wYQCl",
        "outputId": "27a79774-b7c5-411f-d066-7841eaf6ff9e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'twinkle': 1, 'little': 2, 'star': 3, 'wonder': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ['twinkle', 'little', 'star', 'wonder']\n",
        "#   >> aa_index에 더이상 존재하지 않는 단어 추가\n",
        "# [1,2,3,4,5, ??]\n",
        "# Out-Of-Vocabulary : 단어 집합에 없는 단어 >>OOV\n",
        "# aa_index에 'OOV'라는 단어가 있는 자리를 하나 만들고, 그 단어집합에 존재하지 않는 단어를\n",
        "#   OOV의 인덱스로 인코딩"
      ],
      "metadata": {
        "id": "YC9Ye3w_Y18O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa_index['OOV'] = len(aa_index) + 1\n",
        "print(aa_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cKgg_1GaHlA",
        "outputId": "174b9d0d-9bd9-4bda-b489-1162fbc1b8a4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'twinkle': 1, 'little': 2, 'star': 3, 'wonder': 4, 'OOV': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장마다 텍스트 대신에 그 자리에 해당하는 인덱스로 변환\n",
        "# 문장마다 단어로 토큰화 : final_sentence\n",
        "\n",
        "encoding_sentences = []\n",
        "for fs in final_sentence:\n",
        "  encoding_sentence= []\n",
        "  for w in fs:\n",
        "    try:\n",
        "      # 단어 집합에 있는 단어면 해당 단어의 정수를 넣어줌\n",
        "      encoding_sentence.append(aa_index[w])\n",
        "    except KeyError:\n",
        "      # 단어 집합에 없는 단어면 OOV의 정수를 넣어줌\n",
        "      encoding_sentence.append(aa_index['OOV'])\n",
        "  encoding_sentences.append(encoding_sentence)\n",
        "print(encoding_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOzSYOneaKt2",
        "outputId": "4717069c-5ca3-4af0-955f-ffc3043c139f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 2, 3], [4], [5, 5], [5, 5, 5], [1, 1, 2, 3], [4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOh_axMVbBSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}